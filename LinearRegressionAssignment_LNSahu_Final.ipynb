{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement - Identification of factors for varying car price and help Chinese company to enter US market.\n",
    "\n",
    "## Steps to follow\n",
    "- Data Loading and preliminary understanding the data\n",
    "- Data Correction\n",
    "- Understand the data and perform EDA on the data\n",
    "- Split the data into train and test set for Linear regression\n",
    "- Build Model and Perform train set validation\n",
    "- Use different combination of Independent variable to perform validation of train results\n",
    "    - Correlation analysis\n",
    "    - Check P-values for different combination\n",
    "    - Evaluation of R Square, Adjusted R Square and VIF \n",
    "- Indentification of Key Factors based on R2, Adjusted R2 and VIF\n",
    "- Predict on Test set\n",
    "- Compare Train and Test R2-Square value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('max_colwidth', 200)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Displaying Value on top of the bar plots\n",
    "'''\n",
    "def val_on_bar(axs,frmt=' '):\n",
    "    def _single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.2f}'.format(p.get_height()) + frmt\n",
    "            ax.text(_x, _y, value , ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single_plot(ax)\n",
    "    else:\n",
    "        _single_plot(axs)\n",
    "\n",
    "'''\n",
    "Function For Lable Encoding Not used\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def performEncoding(encodingType,df):\n",
    "    \n",
    "    if encodingType=='LE':\n",
    "        le = LabelEncoder() \n",
    "        tmp = df\n",
    "        #Use Label Encoder for These columns as having many categorical variables and having huge class imbalance\n",
    "        # We can scale them using min max scalar to have the values between 0 and 1\n",
    "        # Later by using RFE we can remove the features which are not havint statistical significance\n",
    "        #'enginelocation' - rop from analysis\n",
    "        le_cols = []\n",
    "        for col in le_cols:\n",
    "            le.fit(df[col].drop_duplicates()) \n",
    "            df[col] = le.transform(tmp[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Loading and Understanding of the data\n",
    "- Load the data - Car data and data dictionary for car price\n",
    "- Correct the data corresponding to data dictionary xls - Lot many null columns, unwanted columns as unnamed etc\n",
    "- Create a dataframe containing the column Name, data type and column desription mapped to a single record od car data\n",
    "- Derived Matrix for Car Company Name\n",
    "    - Correct Car Company Name\n",
    "- Explore the car data based on various Plots\n",
    "- Check the data using Pairplot, Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Car Price Data\n",
    "\n",
    "car = pd.read_csv(\"CarPrice_Assignment.csv\")\n",
    "\n",
    "# Load the data dictionary\n",
    "\n",
    "data_dict = pd.read_excel(\"Data Dictionary - carprices.xlsx\")\n",
    "\n",
    "#Correction of Data Dictionary Dataframe\n",
    "\n",
    "data_dict.drop(['Unnamed: 0','Unnamed: 1','Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5',\n",
    "                'Unnamed: 6','Unnamed: 8','Unnamed: 9','Unnamed: 10','Unnamed: 12','Unnamed: 13'],axis=1,inplace=True)\n",
    "data_dict.dropna(axis=0,inplace=True)\n",
    "data_dict[['Col_Name','Decription']] = data_dict\n",
    "data_dict.drop(['Unnamed: 7','Unnamed: 11'],axis=1,inplace=True)\n",
    "data_dict.reset_index(inplace=True)\n",
    "data_dict.drop('index',axis=1,inplace=True)\n",
    "\n",
    "'''\n",
    "Create the Dataframe for dtypes\n",
    "'''\n",
    "d_typ = pd.DataFrame(car.dtypes,columns=['dtypes'])\n",
    "#Reset the Index\n",
    "d_typ = d_typ.reset_index()\n",
    "#Rename the Index column as Column_name\n",
    "d_typ['column_name'] = d_typ['index']\n",
    "d_typ = d_typ[['column_name','dtypes']]\n",
    "\n",
    "'''\n",
    "Create A New Column with First Value/Any one value from the Main Dataframe Lending_loan\n",
    "'''\n",
    "d_typ['Column value'] = car.loc[0].values\n",
    "\n",
    "'''\n",
    "Concat With Data_Dictionary to have a view of the data mapped with Column Name and Description\n",
    "'''\n",
    "\n",
    "data_view = pd.concat([d_typ,data_dict],axis=1)\n",
    "data_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Correction and Preparation\n",
    "### A-  Check For Null and NA values in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadata = False\n",
    "# Check for Null Data in the Car data\n",
    "if len(car.columns[car.isna().any()]) ==0:\n",
    "    print('There is no null data in the Car dataset')\n",
    "else:\n",
    "    print('Deal with Null Data')\n",
    "    nadata =True\n",
    "    \n",
    "if len(car.columns[car.isnull().any()])== 0:\n",
    "    print('There is no NA data in the Car dataset')\n",
    "else:\n",
    "    print('Deal with NA Data')\n",
    "    \n",
    "if nadata:\n",
    "    missing_data = car.isnull().sum()\n",
    "    missing_data.sort_values(inplace=True, ascending=False)\n",
    "    print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-  Data Preparation\n",
    "\n",
    "There is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. \n",
    "\n",
    "\n",
    "As per problem statement, We need to consider only company name as the independent variable for model building.\n",
    "\n",
    "In the dataset, for few of the records the car company name is not correct or short code is used. Correct the car company name to have it nice categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1 - Get Car Company Name\n",
    "'''\n",
    "\n",
    "car['companyName']= car.CarName.apply(lambda x: x.split()[0])\n",
    "car['companyName'].unique()\n",
    "\n",
    "'''\n",
    "2 - Correct Company Name\n",
    "'''\n",
    "#Correct Incorrect Car Compnay Names \n",
    "    # - toyouta : toyota, vokswagen: volkswagen, vw: volkswagen, maxda : mazda, nissan: Nissan, porcshce: porsche\n",
    "\n",
    "company_mapping = {\"companyName\":{ \"maxda\": 'mazda','porcshce':'porsche', \"toyouta\": 'toyota',\n",
    "                                  \"vokswagen\": 'volkswagen',\"vw\": 'volkswagen','Nissan':'nissan'}}\n",
    "car = car.replace(company_mapping)\n",
    "\n",
    "'''\n",
    "3 - Drop Car Name Column and Car_id\n",
    "'''\n",
    "car.drop(['CarName','car_ID'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#'''\n",
    "#4 - Convert Symboling to object type datatype\n",
    "#'''\n",
    "#Its assigned insurance risk rating, A value of +3 indicates that the auto is risky, \n",
    "# -3 that it is probably pretty safe.(Categorical)\n",
    "\n",
    "# car['symboling'] = car['symboling'].astype(object, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are duplicate row records with different price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_price = list(car.columns)\n",
    "except_price.remove('price')\n",
    "car_X = car[except_price]\n",
    "car = car[~car_X.duplicated()] ## Duplicate Features but different amount\n",
    "#Display the dataframe duplicate records \n",
    "car_X[car_X.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding :\n",
    "    - Excluding the Car Model Name and creating Company name column, if we exclude the dependent column 'Price' then there have few duplicate records(identical columns). So remove the duplicate records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection based on 3-Sigma Rule\n",
    "- When checked the dataset, identifed outliners with respect to Horsepower, peakrpm and price by applying 3-sigma rule.\n",
    "- As we are having very minimum data and these may be actual data, with some exceptional Hosepower and Peakrmp configuration or Price may be high due to some other properties, we can not exclude them from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outlier based on horsepower\n",
    "car[car['horsepower'] > 1000].count()\n",
    "\n",
    "#Checking for outlier data based on horsepower\n",
    "car[np.abs(car.horsepower-car.horsepower.mean())>=(3*car.horsepower.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outlier data based on horsepower\n",
    "car[np.abs(car.peakrpm-car.peakrpm.mean())>=(3*car.peakrpm.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outlier data based on price\n",
    "car[np.abs(car.price-car.price.mean())>=(3*car.price.std())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "    - As we will be using MIN_MAX_SCALER, we don't have to treat the Outliers. After min max scaler all the values will be within the range 0 and 1. Hence DO NOT DROP THEM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Exploratory Data Analysis\n",
    "\n",
    "### A - Check the Categorical data - Count of each category and their percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge class imbalance between categories held within a single features such as: \n",
    "    - symboling(-2) has only 1.4% of data\n",
    "    - carbody - hardtop  - 3.9%,convertible 2.9%\n",
    "    - drivewheel- 4wd    - 4.3%\n",
    "    - rear enginelocation - 1.4%\n",
    "    - engine type rotor - 1.9%, dohcv- .4%\n",
    "    - cylindernumber - eight, two, twelve, three are having less than 5% of data in each category\n",
    "    - fuelsystem - spdi, 4bbl, mfi, spfi are having less than 5% of data in each category\n",
    "    - companyName  - dodge, bmw, buick, plymouth, audi, saab, porsche, isuzu, jaguar, alfa-romero, chevrolet, renault, mercury are having less than 5% of data in each category\n",
    "\n",
    "- So, let's first perform EDA on the data then later we will encode the categorical features for Model Building.\n",
    "- We will deal with all the categorical features differently depending on there distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check the Categorical data - Count of each category and their percentages\n",
    "'''\n",
    "cat_car = car.select_dtypes(include=['object'])\n",
    "for col in cat_car.columns:\n",
    "    print(col,':')\n",
    "    print(pd.concat([cat_car[col].value_counts(),cat_car[col].value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "    print('----------------------------------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "- Number of vehicles by company name\n",
    "- Car Insurance Risk Factor - symboling\n",
    "- Aspiration and Fueltype - pie chart\n",
    "- No of doors, Drive wheel and carbody type bar plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car['companyName'].value_counts().nlargest(car['companyName'].count()).plot(kind='bar', figsize=(15,5))\n",
    "plt.title(\"Number of vehicles by company name\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Company Name');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: \n",
    "    - The number of cars under each car company is not sampled uniformly. There is huge class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "car.symboling.value_counts().plot(kind='bar',figsize=(12, 4))\n",
    "plt.title(\"Symboling Bar Plot\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Symboling type');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "car['aspiration'].value_counts().plot.pie(figsize=(12, 4), autopct='%.2f')\n",
    "plt.title(\"Aspiration pie diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Aspiration type');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "car['fueltype'].value_counts().plot.pie(figsize=(12, 4), autopct='%.2f')\n",
    "plt.title(\"Fuel type pie diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Fuel type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: \n",
    "    - The number of cars under each risk factor is not sampled uniformly. There is huge class imbalance.\n",
    "    - Aspiration type 'std' is having 81.73% of the data where as 'turbo' is 18.27\n",
    "    - Fuel Type gas- 90.36 where as diesel-9.64\n",
    "    - We will deal with the categorical variable after performing the analysis on theirdistribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,6))\n",
    "plt.subplot(1,4,1)\n",
    "car['doornumber'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Car doors frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Number of doors');\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "car['drivewheel'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Drive wheel frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Drivewheel Type');\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "car['carbody'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Carbody type frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Carbody type');\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "car['cylindernumber'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Cylinder Number type frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Cylinder Number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "    - Drivewheel, carbody and cylinder do not have even distribution. We need to perform appropriate encoding to use them in model building.\n",
    "    - Fore Wheeldrive and Rear Wheel Drive are more than fur wheel drive\n",
    "    - Sedan and Hatchback are more than other categories\n",
    "    - Four cylindercars are more than other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,6))\n",
    "plt.subplot(1,2,1)\n",
    "car['enginetype'].value_counts().plot(kind='bar')\n",
    "plt.title(\"enginetype frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Engine type');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "car['fuelsystem'].value_counts().plot(kind='bar')\n",
    "plt.title(\"fuelsystem frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Fuel System Type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "    - Car doors can be converted to numerical 4 for 'four' and 2 for 'two'\n",
    "    - Drivewheel, carbody type, engine type and fuel system type are having quite less nos of data for few categories. This is also an example case of undersampling.\n",
    "    - cylinder number can be converted to numerical values i.e four -4, six -6 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "car['curbweight'].hist(bins=5,color='green');\n",
    "plt.title(\"Curb weight histogram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Curb weight');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "car['horsepower'].hist(bins=5,color='blue');\n",
    "plt.title(\"Horsepower histogram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Horsepower');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "car['enginesize'].hist(bins=5,color='blue');\n",
    "plt.title(\"Enginesize histogram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Enginesize');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "    - There are quite less number of cars in higher curbweight, horsepower and enginesize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate and Correlation Analysis\n",
    "- Box Plots - Car distribution based on various categorical features\n",
    "- Grouping of features and checking the mean price\n",
    "\n",
    "##### Price distribution based on Categorical Features - using box plots\n",
    " - Analysis Summary : \n",
    "     - Fuel Type: Diesel cars are on an average higher price than Gas Cars - But High Price Car are of type Gas. Class imbalance problem.\n",
    "     - Turbo aspiration cars are on an average higher price than std Cars - But most of the high price car are of type std. Class imbalance problem. We need more data to know the actual affect.\n",
    "     - Carbody - hardtop cars are on an average higher price than others - But Less Cars of type Hardtop - Class imbalance problem. We need more data to know the actual affect.\n",
    "     - Drivewheel of type 'rwd' affects the increase in Price type -  it seems rear-wheel drive vehicles are, on an average, the most expensive, while 4-wheel and front-wheel are approximately low priced cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to plot box plot and print the number of observation on box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberonboxplot(df,col,ax):\n",
    "    tops = df.groupby([col])['price'].median().sort_values(ascending=True)\n",
    "    nobs = pd.DataFrame(pd.concat([df.groupby([col])['price'].median().sort_values(ascending=True),\n",
    "                 df[col].value_counts()],axis=1))\n",
    "    nobs.sort_values(by='price',ascending=True,inplace=True)\n",
    "    nobs= nobs[col].values\n",
    "    nobs = [str(x) for x in nobs.tolist()]\n",
    "    nobs = [\"n: \" + i for i in nobs]\n",
    "    # Add it to the plot\n",
    "    #print(nobs)\n",
    "    pos = range(len(nobs))\n",
    "    \n",
    "    for tick,label in zip(pos,ax.get_xticklabels()):\n",
    "        ax.text(pos[tick], tops[tick] + 0.03, nobs[tick],\n",
    "                horizontalalignment='center', size='medium', color='black', weight='semibold')\n",
    "        \n",
    "def plotBoxPlot(col):   \n",
    "    # Plot a Horizontal Line at mean price of complete dataset\n",
    "    plt.axhline(y=car['price'].mean(), color='r')\n",
    "\n",
    "    # Order the box plots in acending order of median value\n",
    "    dfm = pd.DataFrame(car[[col,'price']].groupby([col]).median().sort_values(by='price',ascending=True))\n",
    "    l =list(dfm.index.get_values())\n",
    "    ax = sns.boxplot(x=col, y='price', data=car, palette=\"Set1\",order=l)\n",
    "    # Add jitter with the swarmplot function.\n",
    "    sns.swarmplot(x=col , y='price', data=car, color=\"blue\",order=l)\n",
    "\n",
    "    if col!='symboling':\n",
    "        numberonboxplot(car,col,ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature = ['fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','symboling',\n",
    "               'enginetype','cylindernumber','fuelsystem']\n",
    "plt.figure(figsize=(20,25))\n",
    "for i, feature in enumerate(cat_feature):\n",
    "    if(len(cat_feature)%2 ==0):\n",
    "        rows = int(len(cat_feature)/2)\n",
    "    else:\n",
    "        rows = int(len(cat_feature)/2) +1\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    # Draw a horizontal Line - mean amount of price\n",
    "    plt.axhline(y=car['price'].mean(), color='r')\n",
    "    \n",
    "    # Order the box plots in acending order of median value\n",
    "    dfm = pd.DataFrame(car[[feature,'price']].groupby([feature]).median().sort_values(by='price',ascending=True))\n",
    "    l =list(dfm.index.get_values())\n",
    "    \n",
    "    ax = sns.boxplot(x=feature, y='price', data=car, palette=\"Set1\", order=l)\n",
    "    \n",
    "    # Add jitter with the swarmplot function.\n",
    "    sns.swarmplot(x=feature, y='price', data=car, color=\"blue\", order=l)\n",
    "    \n",
    "    # Show Number of Observation On the Box plot\n",
    "    if feature!='symboling':\n",
    "        numberonboxplot(car,feature,ax)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plot based on car company and price distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(car.price.describe())\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.axhline(y=car['price'].mean(), color='r')\n",
    "\n",
    "#Order the box plots in acending order of median value\n",
    "dfm = pd.DataFrame(car[['companyName','price']].groupby(['companyName']).median().sort_values(by='price',ascending=True))\n",
    "l =list(dfm.index.get_values())\n",
    "ax = sns.boxplot(x='companyName', y='price', data=car, palette=\"Set1\",order=l)\n",
    "\n",
    "# Add jitter with the swarmplot function.\n",
    "sns.swarmplot(x='companyName' , y='price', data=car, color=\"blue\",order=l)\n",
    "\n",
    "numberonboxplot(car,'companyName',ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "    - Most Of the features are having overlapping information. \n",
    "    - None of the features alone is explaining the cause of pricing.\n",
    "    - For car companies such as mercury, audi, bmw, jaguar, buick, porsche,volvo - the car with minimum amount is higher than the population mean amount.\n",
    "    - We can use LableEncoding with ascending mean amount for model building.\n",
    "    - Based on the observations, we can't use all the categories for analysis as it may result in wrong conclusion due to high class imbalance\n",
    "    - We need to group few categories and find some derived features to check the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Grouping drivewheel, carbody and getting the mean price, it can be clearly visualized that rear wheel drive is on an average having higher value for all car body type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping results\n",
    "car_gp = car[['drivewheel','carbody','price']]\n",
    "car_gp1 = car_gp.groupby(['drivewheel','carbody'],as_index=False).mean()\n",
    "pivot_by_wheeltype = car_gp1.pivot(index='drivewheel',columns='carbody')\n",
    "print(pivot_by_wheeltype.fillna(0))\n",
    "\n",
    "sns.heatmap(pivot_by_wheeltype, annot=True, fmt='.2f',cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding :\n",
    "    - rwd - Rear wheel drive car on an average higher price than 4 wheel drive and forward wheel drive cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_gp2 = car[['drivewheel','companyName','price']]\n",
    "car_gp2 = car_gp2.groupby(['drivewheel','companyName'],as_index=False).min()\n",
    "car_gp2 = car_gp2.sort_values(by='price', ascending= True)\n",
    "\n",
    "\n",
    "print('Average car price of the whole data set:', car['price'].mean())\n",
    "print('\\n')\n",
    "print('Car Companies who manufactures cars with price much higher than average car price: ')\n",
    "car_gp2[car_gp2['price'] >= car['price'].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B - Understanding the numerical variables\n",
    "- Correlation Analysis of Numerical Columns to identify highly correlated features\n",
    "- Pair plots to check the relationship of Price against other numerical columns\n",
    "- Price vs Numerical Features Pair plot with regression line\n",
    "- Ditribution and box Plot of Car Price\n",
    "- Box Plots or Caterical Fetaures against Car Price\n",
    "- Binning of Car Price/Horsepower based on price Range as Low, Mid and High\n",
    "- Distribution plot based on Car Price Range against other numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the numerical columns only\n",
    "'''\n",
    "cat_car = car.select_dtypes(include=['object'])\n",
    "num_cols = set(car.columns).difference(set(cat_car.columns))\n",
    "num_cols = list(num_cols)\n",
    "\n",
    "#Check The Min, Max, Average of numerical features\n",
    "car[num_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Correlation Analysis Using Heatmap\n",
    "- citympg and highwaympg are highly correlated > .97 and both are having a -ve(almost equal) correlation with Price, Horsepower and engine size. \n",
    "- Carlength is having a high +ve correlation with Carwidth(.84), Carwheel base(.87)\n",
    "- Price is having +v correlation with curbweight, enginesize, horsepower, carwidth, carlength, wheelbase, boreratio\n",
    "- Highly Correlated Independent Features are \n",
    "    - 1) citympg and highwaympg\n",
    "    - 2) curbweight, Carwidth, wheelbase and carlength\n",
    "    - 3) Enginesize and horsepower\n",
    "    - 4) Enginesize, curbweight, wheelbase and carlength\n",
    "\n",
    "\n",
    "We can eleminate few of the features after performing the P-value test, RFE and VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,20))\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(car[num_cols].corr(),annot=True,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Analysis of Numerical features using Pair Plot\n",
    "- It is obvious that Price is having a linear relation ship with folliwng features:\n",
    "    - curbweight \n",
    "    - enginesize\n",
    "    - carlength\n",
    "    - highwaympg\n",
    "    - horsepower\n",
    "    - wheelbase\n",
    "    - carwidth\n",
    "    - citymapg\n",
    "    - bore ratio\n",
    "    \n",
    "- It seems there are two categories based on car compressionratio distribution but Price is not having linear correlation with compression ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features such as peakrpm, stroke, carheight and compressionratio don't seem like having any linear realtionship with Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "#plt.figure(figsize=(24,16))\n",
    "num_cols.remove('price')\n",
    "sns.set(font_scale=1)\n",
    "sns.pairplot(data=car, x_vars=num_cols[0:4], y_vars='price',kind=\"reg\")\n",
    "sns.pairplot(data=car, x_vars=num_cols[4:8], y_vars='price',kind=\"reg\")\n",
    "sns.pairplot(data=car, x_vars=num_cols[8:12], y_vars='price',kind=\"reg\")\n",
    "sns.pairplot(data=car, x_vars=num_cols[12:16], y_vars='price',kind=\"reg\")\n",
    "#sns.pairplot(data=car, x_vars=num_cols[12:16], y_vars='price',kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatter plot of price and engine size\n",
    "sns.lmplot(y='price',x=\"enginesize\", data=car);\n",
    "plt.title('Engine Size vs Price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(y='price',x=\"curbweight\", data=car)\n",
    "plt.title('Curb weight vs Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "    - Price is having a linear relationship with Curbweight and EngineSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(y='citympg',x='curbweight', data=car, fit_reg=True); #hue=\"companyName\",\n",
    "sns.lmplot(y='citympg',x='enginesize', data=car, fit_reg=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings :\n",
    "    - Citympg decreases with increase in enginesize and curbweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x= 'enginesize' , y='price', hue = 'doornumber', data=car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x= 'enginesize' , y='price', hue = 'fuelsystem', data=car)\n",
    "\n",
    "sns.lmplot(x= 'horsepower' , y='price', hue = 'fuelsystem', data=car)\n",
    "\n",
    "sns.lmplot(x= 'enginesize' , y='price', hue = 'symboling', data=car)\n",
    "\n",
    "sns.lmplot(x= 'horsepower' , y='price', hue = 'symboling', data=car)\n",
    "\n",
    "sns.lmplot(x= 'enginesize' , y='price', hue = 'cylindernumber', data=car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    " - Door Number is not contributing significatnt variation to the price\n",
    " - Fuel system and horsepower or enginetype expains the price variation pretty well.\n",
    " - Horse Power and symboling(riskfactor) is explaining some of the price variation but not that clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Distribution and box plot of amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "\n",
    "fig, ax =plt.subplots(1,2, figsize=(10,4),squeeze=False)\n",
    "s1 = sns.distplot(car.price,bins = 10, ax=ax[0][0])\n",
    "#plt.show()\n",
    "\n",
    "s2= sns.boxplot(y=car.price, ax=ax[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) Binning of Car Dataset based on Price and Horsepower and plot bar plots:\n",
    "\n",
    "##### New metrics are derived if applicable and are used for analysis and modelling.\n",
    "\n",
    "- Based on the car price, we can segregate cars into three categorical segments:\n",
    "    - Low\n",
    "    - Mid\n",
    "    - High\n",
    "- Based on horse power, the dataset can be segregated into three bins:\n",
    "    - Low\n",
    "    - Mid\n",
    "    - High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(car[\"horsepower\"]), max(car[\"horsepower\"]), 4)\n",
    "labels = ['Low','Mid','High']\n",
    "car['horsepower_range'] = pd.cut(car.horsepower,bins=bins,labels=labels,include_lowest=True )\n",
    "\n",
    "bins = np.linspace(min(car[\"price\"]), max(car[\"price\"]), 4)\n",
    "labels = ['Low','Mid','High']\n",
    "car['price_range'] = pd.cut(car.price,bins=bins,labels=labels,include_lowest=True)\n",
    "'''\n",
    "Print the count of cars in each category\n",
    "'''\n",
    "# There is not enough data in each category.\n",
    "print(car[\"price_range\"].value_counts())\n",
    "print(car[\"horsepower_range\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- curbweight is correlated with carheight and carwidth so, we can derive new matrices by applying some generic formulas such deriving volume and desity\n",
    "- horsepower and enginesize are correalted hence we can create a new feature as powerper volume size of the engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new features\n",
    "car['volume'] = car['carheight'] * car['carwidth'] * car['carlength']\n",
    "car['density'] = car['curbweight'] / car['volume']\n",
    "car['powerpervolume'] = car['horsepower'] / car['enginesize']\n",
    "\n",
    "# Repopulate the Num_Cols List\n",
    "#num_cols = set(car.columns).difference(set(cat_car.columns))\n",
    "#num_cols = list(num_cols)\n",
    "\n",
    "num_cols.append('volume')\n",
    "num_cols.append('powerpervolume')\n",
    "num_cols.append('density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot bar plots to check the counts based on price and horsepower range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2,1)\n",
    "plt.bar(labels,car[\"price_range\"].value_counts())\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Price Range\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(labels,car[\"horsepower_range\"].value_counts())\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Horsepower Range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v) Distribution Plot Price_range vs Other Numerical Features\n",
    "- The distribution is overlapping hence not that helpful in interpreting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 5\n",
    "plt.figure(figsize=(18,18))\n",
    "for i, feature in enumerate(num_cols):\n",
    "    if len(num_cols) %2 == 0:\n",
    "        rows = int(len(num_cols)/2)\n",
    "    else:\n",
    "        rows = int(len(num_cols)/2) +1\n",
    "\n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.distplot(car[car.price_range=='Low'][feature], bins=bins, color='red', label='L');\n",
    "    sns.distplot(car[car.price_range=='Mid'][feature], bins=bins, color='blue', label='M');\n",
    "    sns.distplot(car[car.price_range=='High'][feature], bins=bins, color='green', label='H');\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vi) Distribution Plot Horsepower_range vs Other Numerical Features\n",
    "- The distribution is overlapping hence not that helpful in interpreting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 5\n",
    "plt.figure(figsize=(15,15))\n",
    "for i, feature in enumerate(num_cols):\n",
    "    if len(num_cols) %2 == 0:\n",
    "        rows = int(len(num_cols)/2)\n",
    "    else:\n",
    "        rows = int(len(num_cols)/2) +1\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.distplot(car[car.horsepower_range=='Low'][feature], bins=bins, color='red', label='L');\n",
    "    sns.distplot(car[car.horsepower_range=='Mid'][feature], bins=bins, color='blue', label='M');\n",
    "    sns.distplot(car[car.horsepower_range=='High'][feature], bins=bins, color='green', label='H');\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Value Analysis - Pearsonr  Correlation Analysis\n",
    "- Approach :\n",
    " - If p-value is  <  0.001: then strong evidence that the correlation is significant.\n",
    " - p-value is  <  0.05: then moderate evidence that the correlation is significant.\n",
    " - p-value is  <  0.1: then weak evidence that the correlation is significant.\n",
    " - p-value is  >  0.1: then no evidence that the correlation is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "for col in num_cols: \n",
    "    pearson_coef, p_value = stats.pearsonr(car[col], car['price'])\n",
    "    \n",
    "    print(col,':')\n",
    "    print(\"The PearsonR between feature and price is {} with a P-value = {}\".format(pearson_coef, round(p_value,5)))\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        print('Correlation between feature and price is statistically significant. P-value less than .001.')\n",
    "    elif p_value < 0.05:\n",
    "        print('Correlation between feature and price is statistically significant.')\n",
    "    elif p_value < 0.1:\n",
    "        print('Correlation between feature and price is statistically weak.')\n",
    "    else:\n",
    "        print('Correlation between feature and price is statistically not significant.')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables creation wherever applicable\n",
    "\n",
    "##### 1 - Dealing with categorical columns having two values : doornumber, aspiration and fueltype. \n",
    "    - Encoding with 0 and 1.\n",
    "    - doornumber is can be converted to numeric value four to 4 and two to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['doornumber'].value_counts(),car['doornumber'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "print(pd.concat([car['aspiration'].value_counts(),car['aspiration'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "print(pd.concat([car['fueltype'].value_counts(),car['fueltype'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "bin_cat = {\"doornumber\":{\"four\": 4, \"two\": 2},\n",
    "           \"aspiration\":{\"std\":1,\"turbo\":0},\n",
    "           \"fueltype\":{\"gas\":1,\"diesel\":0}}\n",
    "\n",
    "car.replace(bin_cat, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 - Dealing with cylindernumber categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['cylindernumber'].value_counts(),car['cylindernumber'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "#Create Dummies for Four, Six, Five  CylinderNumber and Remove other columns\n",
    "car= pd.get_dummies(car,columns=['cylindernumber'])\n",
    "car.drop(['cylindernumber_eight','cylindernumber_two','cylindernumber_twelve','cylindernumber_three'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see there are less than 5 % of data for few categories, we can't use them for modelling what we will do, we first apply dummy to create individual columns for each cylinder type then drop the columns correspoding to cylinders :eight (2.54 %),two (1.52%),twelve 0.51% and three  0.51%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 - Dealing with carbody categorical column\n",
    "\n",
    "- As you can see there are less than 5 % of data for few categories, we can't use them for modelling. What we will do, we first apply dummy to create individual columns for each carbody type then drop the columns correspoding to carbody :hardtop(3.55%) and convertible(2.54%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['carbody'].value_counts(),car['carbody'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "# Create Dummies and drop hardtop and convertible\n",
    "car= pd.get_dummies(car,columns=['carbody'])\n",
    "car.drop(['carbody_hardtop','carbody_convertible'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 - Dealing with drivewheel categorical column\n",
    "- Remove 4wd category as having only 4.57% of the data after performing one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['drivewheel'].value_counts(),cat_car['drivewheel'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "# Create Dummies and Drop 4wd\n",
    "car= pd.get_dummies(car,columns=['drivewheel'])\n",
    "car.drop(['drivewheel_4wd'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 - Dealing with enginelocation categorical column\n",
    "- Remove rear enginelocation category as having only 1.02% of the data by performing one hot encoder i.e front =1, rear =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['enginelocation'].value_counts(),car['enginelocation'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "# Update EngineLocation\n",
    "bin_cat = {\"enginelocation\":{\"front\": 1, \"rear\": 0}}\n",
    "car.replace(bin_cat, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6 - Dealing with enginetype categorical column\n",
    "\n",
    "- The engine type of a vehicle states how the engine is configured or its operations in term of design of valves, cam shaft and cylinders. Here there are seven engine types: \n",
    "    - ohc (OverHead Cam) - 73.60%\n",
    "    - dohc (Dual OverHead Cam) - 5.08%\n",
    "    - dohcv (Dual OverHead Cam and Valve) - 0.51%\n",
    "    - l (L engine), ohc (OverHead Cam) - 5.58%\n",
    "    - ohcf (OverHead Cam and Valve F engine) - 7.11%\n",
    "    - ohcv (OverHead Cam and Valve) - 6.6%\n",
    "    - rotor (Rotary engine) - 1.52%\n",
    "    \n",
    "- Here 'dohcv' is having one records and its withing the range of 'dohc' hence update the category type to 'dohc'\n",
    "- After applying Onehotencoding remove the column correspoding to 'rotor' enginetype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['enginetype'].value_counts(),car['enginetype'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "plt.figure(figsize=(10,5))\n",
    "# Box Plot See the dictribution for each Engine Type\n",
    "plotBoxPlot('enginetype')\n",
    "\n",
    "# Update dohcv to dohc\n",
    "eng_upd = {\"enginetype\":{\"dohcv\": \"dohc\"}}\n",
    "car.replace(eng_upd, inplace=True)\n",
    "\n",
    "# Create Dummies\n",
    "car= pd.get_dummies(car,columns=['enginetype'])\n",
    "\n",
    "# Drop Rotor\n",
    "car.drop(['enginetype_rotor'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7 - Dealing with fuelsystem categorical column\n",
    "- Fuel System 1bbl and 2bbl the dictibution is almost same - Lets update them as 2bbl\n",
    "- Fuel system spfi having only 1 data and price is just higher than the mean of spdi. Hence update it to spdi. \n",
    "- Fuel system 4bbl and mfi having only 2 and 1 data respectively and have price just less than or equal to mean of idi. Hence update them to idi\n",
    "- Apply OnehotEncoding and then drop the column correspoding to spdi(as having least no of values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['fuelsystem'].value_counts(),car['fuelsystem'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "# Plot box plot for fuelsystem type distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "plotBoxPlot('fuelsystem')\n",
    "\n",
    "# Update dohcv to dohc\n",
    "fuel_upd = {\"fuelsystem\":{\"1bbl\": \"2bbl\",\"spfi\":\"spdi\",\"4bbl\":\"idi\",\"mfi\":\"idi\"}}\n",
    "car.replace(fuel_upd, inplace=True)\n",
    "\n",
    "#Check the dictribution after update\n",
    "plt.figure(figsize=(10,5))\n",
    "plotBoxPlot('fuelsystem')\n",
    "\n",
    "# Create Dummies\n",
    "car= pd.get_dummies(car,columns=['fuelsystem'])\n",
    "\n",
    "# Drop Rotor\n",
    "car.drop(['fuelsystem_spdi'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8 - Split Engine Size to Low Mid High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "# Plot a Horizontal Line at mean price of complete dataset\n",
    "plt.axhline(y=car['enginesize'].mean(), color='r')\n",
    "\n",
    "# Order the box plots in acending order of median value\n",
    "dfm = pd.DataFrame(car[['companyName','enginesize']].groupby(['companyName']).median().sort_values(by='enginesize',ascending=True))\n",
    "l =list(dfm.index.get_values())\n",
    "ax = sns.boxplot(x='companyName', y='enginesize', data=car, palette=\"Set1\",order=l)\n",
    "# Add jitter with the swarmplot function.\n",
    "sns.swarmplot(x='companyName' , y='enginesize', data=car, color=\"blue\",order=l)\n",
    "\n",
    "#if col!='symboling':\n",
    "    #numberonboxplot(car,'companyName',ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bins as Engine Size\n",
    "bins = [0,100,150,500]\n",
    "labels = [0,1,2]\n",
    "car['engine_range'] = pd.cut(car.enginesize,bins=bins,labels=labels,include_lowest=True )\n",
    "plt.figure(figsize=(10,6))\n",
    "plotBoxPlot('engine_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9 - Split Horse Power to High and Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "# Plot a Horizontal Line at mean price of complete dataset\n",
    "plt.axhline(y=car['horsepower'].mean(), color='r')\n",
    "\n",
    "# Order the box plots in acending order of median value\n",
    "dfm = pd.DataFrame(car[['companyName','horsepower']].groupby(['companyName']).median().sort_values(by='horsepower',ascending=True))\n",
    "l =list(dfm.index.get_values())\n",
    "ax = sns.boxplot(x='companyName', y='horsepower', data=car, palette=\"Set1\",order=l)\n",
    "# Add jitter with the swarmplot function.\n",
    "sns.swarmplot(x='companyName' , y='horsepower', data=car, color=\"blue\",order=l)\n",
    "\n",
    "#if col!='symboling':\n",
    "    #numberonboxplot(car,'companyName',ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bins as per hosrse power and car type distribution\n",
    "bins = [0,150,300]\n",
    "labels = [0,1]\n",
    "car['horsepower_range'] = pd.cut(car.horsepower,bins=bins,labels=labels,include_lowest=True )\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plotBoxPlot('horsepower_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 - Dealing with companyName categorical column\n",
    "- Apply Dummy to do one hot encoding on company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([car['companyName'].value_counts(),car['companyName'].\n",
    "                 value_counts(normalize=True).apply(lambda x: round(x*100,2))],axis=1))\n",
    "\n",
    "# Plot box plot for fuelsystem type distribution\n",
    "plt.figure(figsize=(20,15))\n",
    "plotBoxPlot('companyName')\n",
    "\n",
    "\n",
    "# Keep a Back Up Copy for further Model Testing as we may need to encode based on car company name\n",
    "carbackup = car\n",
    "\n",
    "# Create Dummies\n",
    "car= pd.get_dummies(car,columns=['companyName'],drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns price_range\n",
    "car.drop(['price_range'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "### Approach:\n",
    "    - Train test split\n",
    "    - Build different models with complete data to understand the p-value, VIF\n",
    "    - Perform RFE to further eliminate features\n",
    "    - Identify the features for final model building\n",
    "    - Train the model with Train Set\n",
    "    - Check the R2 and Adjusted R2 on both train and test set\n",
    "    - Perform Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def prepareTrainTestSets(car,randomness=42):\n",
    "    '''\n",
    "    Step 1: Train Test Split\n",
    "    '''    \n",
    "    np.random.seed(0)\n",
    "    df_train, df_test = train_test_split(car, train_size = 0.8, test_size = 0.2, random_state = randomness)\n",
    "\n",
    "    '''\n",
    "    Step 2: Scale the training data\n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    all_cols = list(car.columns)\n",
    "    df_train[all_cols] = scaler.fit_transform(df_train[all_cols])\n",
    "\n",
    "    '''\n",
    "    Step 3: Create the X_train and y_train\n",
    "    '''\n",
    "    y_train = df_train.pop('price')\n",
    "    X_train = df_train\n",
    "        \n",
    "    '''\n",
    "    Setp 4: Scale Test Set and create the X_test and y_test\n",
    "    '''\n",
    "    # Scale Test Set\n",
    "    df_test[all_cols] = scaler.transform(df_test[all_cols])\n",
    "    y_test = df_test.pop('price')\n",
    "    X_test = df_test\n",
    "    \n",
    "    return y_train,X_train,y_test,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Train and Test Dataframe\n",
    "y_train,X_train,y_test,X_test = prepareTrainTestSets(car)\n",
    "\n",
    "# Check the shape\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Test Evaluations \n",
    "\n",
    "### Model 1: Using Sklearn LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Train the Model the model using Training Set\n",
    "'''\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "#num_cols.remove('price')\n",
    "\n",
    "# fit the model to the training data\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Evaluation Using R-Square Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Predict and Evaluate the training MSE and R-Square\n",
    "'''\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Make Prediction Using the test set\n",
    "y_hat = lr.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_hat)\n",
    "r_squared = r2_score(y_train, y_hat)\n",
    "\n",
    "print('Model 1 Evaluation:')\n",
    "\n",
    "print('\\tTrain Mean_Squared_Error :' ,mse)\n",
    "print('\\tTrain R_square_value :',r_squared)\n",
    "\n",
    "'''\n",
    "Step 3: Predict and Evaluate the training MSE and R-Square\n",
    "'''\n",
    "y_hat_test = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat_test)\n",
    "r_squared = r2_score(y_test, y_hat_test) \n",
    "\n",
    "print('\\tTest Mean_Squared_Error :' ,round(mse,3))\n",
    "print('\\tTest R_square_value :',round(r_squared,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding: \n",
    "    - When all the features are used, the model is slightly overfitting i.e. it is performing well on train data but not so good in test data.\n",
    "    - So let's build another model with all the features but using Statsmodel OLS library to get the P-Values for each feature. Before that, let's identify 15 significant features using RFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE - Recursive feature Elemination\n",
    "\n",
    "**Function** To Select first the 15 features which seems significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE and LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def getColForAnalysisUsingREF(X_train, y_train, featureCount,colstodrop):\n",
    "    # Running RFE with the output number of the variable equal to 20\n",
    "    lm = LinearRegression()\n",
    "    X_train_n = X_train.drop(colstodrop,axis=1)\n",
    "    lm.fit(X_train_n, y_train)\n",
    "\n",
    "    rfe = RFE(lm, featureCount)             # running RFE\n",
    "    rfe = rfe.fit(X_train_n, y_train)\n",
    "\n",
    "    # Print the Output of RFE\n",
    "    print('RFE Output:\\n' , list(zip(X_train_n.columns,rfe.support_,rfe.ranking_)))\n",
    "\n",
    "    # Create the columns to include the significant features identified by RFE\n",
    "    colsForAnalysis = []\n",
    "    for i,j,k in list(zip(X_train_n.columns,rfe.support_,rfe.ranking_)):\n",
    "        if j:\n",
    "            colsForAnalysis.append(i)\n",
    "    return colsForAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: \n",
    "- The test R-Square 0.87 is much lower than the Train R-Square 0.97. This is a case of slight overfitting i.e. the model is fitting the training data but unable to predict on test data. Hence we need to Check p-value and vif for selecting the features.\n",
    "- Let's tune the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Model Building using statsmodel OLS and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def buildModel(colstodrop,y_train,X_train,y_test,X_test):\n",
    "    \n",
    "    #y_train,X_train,y_test,X_test = prepareTrainTestSets(car)\n",
    "    \n",
    "    '''\n",
    "    Step 1: Train the model\n",
    "    '''\n",
    "    if not colstodrop:\n",
    "        X_m = X_train\n",
    "    else:\n",
    "        X_m = X_train.drop(colstodrop,axis=1)\n",
    "    X_m = sm.add_constant(X_m)\n",
    "    # print('shape X_m after constant:',X_m.shape)\n",
    "    lm = sm.OLS(y_train,X_m).fit()\n",
    "    #print(lm.summary(),'\\n') \n",
    "    y_hat_train = lm.predict(X_m)\n",
    "\n",
    "    '''\n",
    "    Step 2: Get the VIF\n",
    "    '''\n",
    "    # Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "    vif = pd.DataFrame()\n",
    "    if not colstodrop:\n",
    "        X_m = X_train\n",
    "    else:\n",
    "        X_m = X_train.drop(colstodrop,axis=1)\n",
    "    if len(X_m.columns) > 1:\n",
    "        vif['Features'] = X_m.columns\n",
    "        vif['VIF'] = [variance_inflation_factor(X_m.values, i) for i in range(X_m.shape[1])]\n",
    "        vif['VIF'] = round(vif['VIF'], 2)\n",
    "        vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "        #print(vif)\n",
    "    \n",
    "    '''\n",
    "    Step 3: Predict On Test Set\n",
    "    '''\n",
    "    #Predict on test data and Evaluate\n",
    "    # print('shape X_test before constant:',X_test.shape)\n",
    "    if not colstodrop:\n",
    "        X_test_m = X_test\n",
    "    else:\n",
    "        X_test_m = X_test.drop(colstodrop,axis=1)\n",
    "    # print('shape X_test_m before constant:',X_test_m.shape)\n",
    "    X_test_m = sm.add_constant(X_test_m, has_constant='add')\n",
    "    # print('shape X_test_m after constant:',X_test_m.shape)\n",
    "    y_hat = lm.predict(X_test_m)\n",
    "    mse = mean_squared_error(y_test, y_hat)\n",
    "    r_squared = r2_score(y_test, y_hat) \n",
    "    \n",
    "    '''\n",
    "    Step 4: Print the Results\n",
    "    '''\n",
    "    print('Training Results :')\n",
    "    print('F-statistics :', lm.fvalue)\n",
    "    print('R-Squared :', lm.rsquared)\n",
    "    print('Adj R-Squared :', lm.rsquared_adj)\n",
    "    print('\\n')\n",
    "    print('Testing Results :')\n",
    "    print('Test Mean_Squared_Error :' ,mse)\n",
    "    print('Test R_square_value :',r_squared)\n",
    "    print('\\n')\n",
    "    \n",
    "    '''\n",
    "    Step 5: Create the View for P-value, VIF and Coefficients\n",
    "    '''\n",
    "    \n",
    "    #Create a dataframe to hold the p-values \n",
    "    d = lm.pvalues.apply(lambda x:round(x,3)).to_dict()\n",
    "    p_val = pd.DataFrame()\n",
    "    p_val['Features'] = list(d.keys())\n",
    "    p_val['p-value'] = list(d.values())\n",
    "    \n",
    "    ##Create a dataframe to hold the p-values and VIF by mergeing the dataframes\n",
    "    # For Const the VIF will be zero. Or in case of single variable, VIF won''t be calculated.\n",
    "    if vif.empty ==False:\n",
    "        p_val = p_val.merge(vif, on ='Features', how='left')\n",
    "        p_val.fillna(0,inplace=True)\n",
    "        p_val.sort_values(by='p-value',ascending=False,inplace=True)\n",
    "    \n",
    "    print('P-value and VIF values for the features:')\n",
    "    \n",
    "    #Create and dataframe for Co-effs\n",
    "    c = lm.params.apply(lambda x:round(x,3)).to_dict()\n",
    "    coef = pd.DataFrame()\n",
    "    coef['Features'] = list(c.keys())\n",
    "    coef['Coeffs'] = list(c.values())\n",
    "    pvif = p_val.merge(coef, on ='Features', how='left')\n",
    "    pvif.fillna(0,inplace=True)\n",
    "    \n",
    "    print(tabulate(pvif, headers='keys', tablefmt='psql'))\n",
    "    \n",
    "    return lm, pvif, y_hat_train, y_hat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Using StatsModel Oridnary Least Square to check p-values for all the features\n",
    "- Customised output for readability and better\n",
    "- P-value, VIF and Co-effs on a single dataframe\n",
    "- Use tabulate libaracy to display the dataframe in  much clearer way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Multilinear Regression using All Columns without RFE\n",
    "y_train,X_train,y_test,X_test = prepareTrainTestSets(car)\n",
    "\n",
    "colstodrop = []\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Drop Highly Correlated COlumns\n",
    "# From Earlier Heat map we identified that following features are highly correlated\n",
    "colstodrop = ['volume','carlength','citympg','curbweight']\n",
    "\n",
    "# Identify columns using RFE\n",
    "colsForAnalysis = getColForAnalysisUsingREF(X_train, y_train,20,colstodrop)\n",
    "print('\\n')\n",
    "print('Significant columns after performing RFE :\\n' , colsForAnalysis)\n",
    "\n",
    "# Find the columns to drop excluding the columns identified as significant by RFE \n",
    "colstodrop = list(set(X_train.columns).difference(set(colsForAnalysis)))\n",
    "\n",
    "colstodrop.append('volume')\n",
    "colstodrop.append('carlength')\n",
    "colstodrop.append('citympg')\n",
    "colstodrop.append('curbweight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Tuning:\n",
    "    - The model is not performing well n the test set. This is clear case of overfitting.\n",
    "    - Drop the columns having High P-value and High VIF\n",
    "    - Then High P and Low VIF remove these first\n",
    "    - Then Low P and High VIF remove these after the ones above \n",
    "    - check P-values and Correlation coefficient to select the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 :Including the features identified by REF and perform Interations\n",
    "\n",
    "##### Including the features identified in RFE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to hold the rediuals\n",
    "train_res = pd.DataFrame()\n",
    "test_res = pd.DataFrame()\n",
    "\n",
    "# Perform the Train and Test\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m0'] = y_train - y_hat_train\n",
    "test_res['m0'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings: Seems bit overfitted on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Correlation between numerical features:\n",
    "\n",
    "Remember: Correlation is not Causation. So, need to exclude the features considering P-Value and Vif along with correlation.\n",
    " - Due to high correlation We have already removed these features before performing RFE \n",
    "     - volume\n",
    "     - carlength\n",
    "     - citympg\n",
    "     - curbweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Excluding the other features which are not identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,15))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(X_train.drop(colstodrop,axis=1).corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion From Heatmap:\n",
    "From the HeatMap, we can conclude that:\n",
    "        - companyName_subaru highly correlated with enginetype_ohcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Remove the Highly Correlated Fetaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Remove the Highly Correlated Fetaures\n",
    "colstodrop.append('companyName_subaru')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m1'] = y_train - y_hat_train\n",
    "test_res['m1'] = y_test - y_hat_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rebuild the Model by excluding the features having high p-value and vif\n",
    "- Remove the features:\n",
    "    - having high P-values p > .05 (Priority 1)\n",
    "    - having high p-value and high vif(priority 2)\n",
    "    - low p-value and high VIF(priority 3)\n",
    "\n",
    "Repreat the iteration untill we have p-value less than 5% and VIF less than 5 for the selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 1: Drop powerpervolume high p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('powerpervolume')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m2'] = y_train - y_hat_train\n",
    "test_res['m2'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 2: Drop companyName_renault high P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('companyName_renault')\n",
    "\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "# Populate the Residuals\n",
    "train_res['m3'] = y_train - y_hat_train\n",
    "test_res['m3'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 3: Drop enginetype_ohcf as very high P-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('enginetype_ohcf')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "# Populate the Residuals\n",
    "train_res['m4'] = y_train - y_hat_train\n",
    "test_res['m4'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 4: Drop enginelocation high VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('enginelocation')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m5'] = y_train - y_hat_train\n",
    "test_res['m5'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 5: Drop cylindernumber_six p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('cylindernumber_six')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m6'] = y_train - y_hat_train\n",
    "test_res['m6'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 6: Drop wheelbase high VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('wheelbase')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m7'] = y_train - y_hat_train\n",
    "test_res['m7'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Dropping wheelbase performance decreases hnece keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.remove('wheelbase')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m7'] = y_train - y_hat_train\n",
    "test_res['m7'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 7: Drop carwidth high VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('carwidth')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m8'] = y_train - y_hat_train\n",
    "test_res['m8'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 8: Drop density high VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('density')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m9'] = y_train - y_hat_train\n",
    "test_res['m9'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 9:Drop companyName_peugeot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('companyName_peugeot')\n",
    "#colstodrop.remove('fueltype')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m10'] = y_train - y_hat_train\n",
    "test_res['m10'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 10: Include companyName_honda high P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('companyName_honda')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m11'] = y_train - y_hat_train\n",
    "test_res['m11'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 11: Exclude boreratio high P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.append('boreratio')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m12'] = y_train - y_hat_train\n",
    "test_res['m12'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interation 12: Include enginesize from exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop.remove('enginesize')\n",
    "lm,pvif,y_hat_train,y_hat_test = buildModel(colstodrop,y_train,X_train,y_test,X_test)\n",
    "\n",
    "# Populate the Residuals\n",
    "train_res['m13'] = y_train - y_hat_train\n",
    "test_res['m13'] = y_test - y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_res.columns\n",
    "plt.figure(figsize=(25,30))\n",
    "for i, col in enumerate(cols):\n",
    "    \n",
    "    if len(cols) %2 == 0:\n",
    "        rows = int(len(num_cols)/2)\n",
    "    else:\n",
    "        rows = int(len(num_cols)/2) +1\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)    \n",
    "    ax = sns.distplot(train_res[col],bins = 20)\n",
    "    ax.set(xlabel='Train Errors in model:' + col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: \n",
    "- Error is uniformly distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_train,(y_train - y_hat_train))\n",
    "fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize=18)                          # X-label\n",
    "plt.ylabel('y_pred', fontsize=16)                          # Y-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test,(y_test - y_hat_test))\n",
    "fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize=18)                          # X-label\n",
    "plt.ylabel('y_pred', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no such pattern identified from the scatter plt. So we can infer that the data is linear and there is no Heteroskedasticity issue from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "From the HeatMap, we can conclude that:\n",
    "        - Wheel base , Car Length , Car Width are highly correlated.\n",
    "        - Highway mpg and city mpg are highly correlated.\n",
    "        - Compression ratio and fuel type are correlated\n",
    "        - Engine size, Curb weight and horse power are also correlated\n",
    "\n",
    "\n",
    "From the Scatter/Pair plots, features which have stronger relationship with price are:\n",
    "    - 1. Curb-Weight\n",
    "    - 2. Engine-Size\n",
    "    - 3. Horsepower\n",
    "    - 4. Mpg(City / Highway mpg)\n",
    "    - 5. Lenght/ Width\n",
    "    - 6. Engine Type\n",
    "    - 7. Fuel System\n",
    "    - 8. Number Of Cylinders - 4 or 6\n",
    "\n",
    "#### Identification of factors for varying car price and help Chinese company to enter US market:\n",
    "- Enginesize, curb-weight, horsepower - These parameters have positive relationship with car price.\n",
    "- Mileage per gallon: As the MPG increases the car price decreases. It means, MPG has a -ve relationship with Horsepower, Enginesize, and Curb weight. It mean, more powerful is the engine, it will consume more Diesel/Gas. Hence the mileage decreases with increase in engine size.\n",
    "- Fuel Type Diesel Cars are manufactured more in comparision with Gas cars.\n",
    "- Std aspirations type are more in the market when compared against the Turbo aspiration type.\n",
    "- Forward Wheel Drive(Fwd) cars are manuafactured more over the 4 Wheel Drive and Rear Wheel Drive Cars. Fwd cars are almost are comparatively lower priced than Rwd cars.\n",
    "- MPFI Fuel System car is manuafactured more and It is bit costlier than other fule systems. 2bbl fuel system is the 2nd highest manuafactured fuel system type and it is reasonably priced. \n",
    "- Engineype OHC is manufactured more when compared against other car engine types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Report Explaining the reason for pricing are:\n",
    "For these the P-value and VIF is less tha 5%\n",
    "- companyName_plymouth\n",
    "- companyName_dodge\n",
    "- companyName_mitsubishi\n",
    "- companyName_toyota\n",
    "- wheelbase\n",
    "- horsepower\n",
    "- companyName_bmw\n",
    "- companyName_buick\n",
    "- companyName_porsche\n",
    "\n",
    "##### Training Results :\n",
    "    - F-statistics : 140.05938256007505\n",
    "    - R-Squared : 0.8955620583731947\n",
    "    - Adj R-Squared : 0.8891678986817576\n",
    "\n",
    "- Testing Results :\n",
    "    - Test Mean_Squared_Error : 0.014222211103648302\n",
    "    - Test R_square_value : 0.8761129569542494\n",
    "\n",
    "**Further We Can Include EngineSize and CurbWeight To Explain More Variation explaining the pricing detail**.\n",
    "\n",
    "- After including EngineSize:\n",
    "\n",
    "- Training Results :\n",
    "    - F-statistics : 137.07329010374917\n",
    "    - R-Squared : 0.9037404674876298\n",
    "    - Adj R-Squared : 0.897147348822399\n",
    "\n",
    "- Testing Results :\n",
    "    - Test Mean_Squared_Error : 0.011584866326137502\n",
    "    - Test R_square_value : 0.8990863781471152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
